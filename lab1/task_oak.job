#!/bin/bash
#SBATCH --job-name=lab1-oak
#SBATCH --partition=debug
#SBATCH --nodes=2
#SBATCH --exclusive
#SBATCH --time=00:15:00
#SBATCH --output=slurm-%j.out

cd "${SLURM_SUBMIT_DIR:-.}"
mkdir -p results

make

echo "=== test mpiexec -np 2 ./benchmark (sweep) ==="
mpiexec -np 2 ./benchmark
echo "=== end test ==="

# Получаем имена выделенных узлов
NODE1=$(scontrol show hostnames "$SLURM_NODELIST" | head -1)
NODE2=$(scontrol show hostnames "$SLURM_NODELIST" | tail -1)

# Создаем файл hosts.txt с явным указанием слотов, чтобы OpenMPI не ругался
echo "$NODE1 slots=8" > hosts.txt
echo "$NODE2 slots=8" >> hosts.txt

# Создаем файл hosts_single.txt только с первым узлом (для тестов памяти и QPI)
echo "$NODE1 slots=8" > hosts_single.txt

# Уровень сети (InfiniBand): раскидываем процессы по двум разным узлам
echo "level,m_bytes,t_sec" > results/oak_network.csv
mpiexec -np 1 --host "$NODE1" ./benchmark : -np 1 --host "$NODE2" ./benchmark | awk '{print "network," $0}' >> results/oak_network.csv

# Уровень ОЗУ: оба процесса на первом узле, жесткая привязка к процессору 0
echo "level,m_bytes,t_sec" > results/oak_memory.csv
mpiexec -np 2 -machinefile hosts_single.txt numactl --cpunodebind=0 --membind=0 ./benchmark | awk '{print "memory," $0}' >> results/oak_memory.csv

# Уровень QPI: оба на первом узле, но первый процесс на процессоре 0, второй на процессоре 1
echo "level,m_bytes,t_sec" > results/oak_qpi.csv
mpiexec -np 1 -machinefile hosts_single.txt numactl --cpunodebind=0 --membind=0 ./benchmark : -np 1 -machinefile hosts_single.txt numactl --cpunodebind=1 --membind=1 ./benchmark | awk '{print "qpi," $0}' >> results/oak_qpi.csv

echo "Done. Results in results/"