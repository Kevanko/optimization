#!/bin/bash
#SBATCH --job-name=lab1-oak
#SBATCH --partition=debug
#SBATCH --nodes=2
#SBATCH --exclusive
#SBATCH --time=00:15:00
#SBATCH --output=slurm-%j.out

cd "${SLURM_SUBMIT_DIR:-.}"
mkdir -p results

module load mpi || true
make

echo "=== test mpiexec -np 2 ./benchmark (sweep) ==="
mpiexec -np 2 ./benchmark
echo "=== end test ==="

# Получаем имена узлов, которые нам выделил SLURM
NODE1=$(scontrol show hostnames "$SLURM_NODELIST" | head -1)
NODE2=$(scontrol show hostnames "$SLURM_NODELIST" | tail -1)

# Уровень сети: принудительно кидаем процессы на разные узлы
echo "level,m_bytes,t_sec" > results/oak_network.csv
mpiexec -np 2 --host "$NODE1,$NODE2" ./benchmark | awk '{print "network," $0}' >> results/oak_network.csv

# Уровень ОЗУ: оба процесса на 1 узле, жестко привязаны к нулевому сокету (numanode 0)
echo "level,m_bytes,t_sec" > results/oak_memory.csv
mpiexec -np 2 --host "$NODE1,$NODE1" numactl --cpunodebind=0 --membind=0 ./benchmark | awk '{print "memory," $0}' >> results/oak_memory.csv

# Уровень QPI: оба на 1 узле, но разнесены по разным сокетам (через двоеточие для MPICH)
echo "level,m_bytes,t_sec" > results/oak_qpi.csv
mpiexec -np 1 --host "$NODE1" numactl --cpunodebind=0 --membind=0 ./benchmark : -np 1 --host "$NODE1" numactl --cpunodebind=1 --membind=1 ./benchmark | awk '{print "qpi," $0}' >> results/oak_qpi.csv

echo "Done. Results in results/"