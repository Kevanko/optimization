#!/bin/sh
# Lab 1: снять замеры на Pine. Результаты: results/pine_*.csv
# На Pine: sbatch task_pine.job  (make выполнится в job). Потом скопировать results/ к себе и make plot локально.

#SBATCH --job-name=lab1
#SBATCH --partition=2288
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=1
#SBATCH --time=00:15:00
#SBATCH --output=slurm-%j.out

cd "${SLURM_SUBMIT_DIR:-.}"
mkdir -p results

# Загрузить MPI и собрать (на Pine перед sbatch можно сделать: module load mpi)
module load mpi 2>/dev/null || module load openmpi 2>/dev/null || true
make

SIZES="4096 16384 65536 262144 1048576 2097152 4194304 8388608"
N=100

# Проверка: один запуск в лог (если в CSV пусто — смотри вывод ниже)
echo "=== test mpiexec -np 2 ./benchmark 4096 2 ==="
mpiexec -np 2 ./benchmark 4096 2
echo "=== end test ==="

# network: 2 узла
echo "level,m_bytes,t_sec" > results/pine_network.csv
for m in $SIZES; do
  out=$(mpiexec -np 2 ./benchmark "$m" "$N" 2>/dev/null | head -1)
  [ -n "$out" ] && echo "network,$out" >> results/pine_network.csv
done

# memory и qpi: 1 узел, 2 процесса (mpiexec с привязкой к одному узлу)
echo "level,m_bytes,t_sec" > results/pine_memory.csv
echo "level,m_bytes,t_sec" > results/pine_qpi.csv
for m in $SIZES; do
  out=$(mpiexec -np 2 --map-by ppr:2:node ./benchmark "$m" "$N" 2>/dev/null | head -1)
  [ -n "$out" ] && echo "memory,$out" >> results/pine_memory.csv
  out=$(mpiexec -np 2 --map-by ppr:2:node ./benchmark "$m" "$N" 2>/dev/null | head -1)
  [ -n "$out" ] && echo "qpi,$out" >> results/pine_qpi.csv
done

echo "Done. Results in results/"
