#!/bin/sh
#SBATCH --job-name=lab1-pine
#SBATCH --partition=2288
#SBATCH --nodes=2
#SBATCH --exclusive    
#SBATCH --time=00:15:00
#SBATCH --output=slurm-%j.out

cd "${SLURM_SUBMIT_DIR:-.}"
mkdir -p results

module load mpi 2>/dev/null || module load openmpi 2>/dev/null || true
make

echo "=== test mpiexec -np 2 ./benchmark (sweep) ==="
mpiexec -np 2 ./benchmark
echo "=== end test ==="

# Уровень сети: 2 узла, строго по 1 процессу на каждый узел
echo "level,m_bytes,t_sec" > results/pine_network.csv
mpiexec -np 2 --map-by ppr:1:node ./benchmark 2>/dev/null | awk '{print "network," $0}' >> results/pine_network.csv

# Уровень ОЗУ: оба процесса жестко сажаются на 1 сокет (процессор) одного узла
echo "level,m_bytes,t_sec" > results/pine_memory.csv
mpiexec -np 2 --map-by ppr:2:socket ./benchmark 2>/dev/null | awk '{print "memory," $0}' >> results/pine_memory.csv

# Уровень QPI: процессы сажаются на разные сокеты (по 1 на сокет) одного узла
echo "level,m_bytes,t_sec" > results/pine_qpi.csv
mpiexec -np 2 --map-by ppr:1:socket ./benchmark 2>/dev/null | awk '{print "qpi," $0}' >> results/pine_qpi.csv

echo "Done. Results in results/"